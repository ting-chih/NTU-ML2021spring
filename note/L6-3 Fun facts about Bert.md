## YT  
  * https://www.youtube.com/watch?v=ExXA05i8DEQ&feature=youtu.be  

## Why does BERT work?
 * The tokens with similar meaning have similar embedding.  
 * Context is considered.  
 * Contextualized word embedding: You shall know a word by the company it keeps.  


## Multi-lingual BERT  
 * Training a BERT model by many different languages.  

## Cross-lingual Alignment  
 * Using MRR score to evaluate alignment.  
 * Mean Reciprocal Rank(MRR): higher MRR, better alignment  

## The amount of training data is critical for alignment.  
