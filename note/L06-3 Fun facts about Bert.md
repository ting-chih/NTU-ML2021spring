## YT  
  * https://www.youtube.com/watch?v=ExXA05i8DEQ&feature=youtu.be  

## Why does BERT work?
 * The tokens with similar meaning have similar embedding.  
 * Context is considered.  
 * Contextualized word embedding: You shall know a word by the company it keeps.  
![Image of Yaktocat](https://github.com/ting-chih/NTU-ML2021spring/blob/main/image/BERT%20apple.png)  

## Multi-lingual BERT  
 * Training a BERT model by many different languages.  

## Cross-lingual Alignment  
 * Using MRR score to evaluate alignment.  
 * Mean Reciprocal Rank(MRR): higher MRR, better alignment  
![Image of Yaktocat](https://github.com/ting-chih/NTU-ML2021spring/blob/main/image/cross%20lingual.png)  

## The amount of training data is critical for alignment.  
![Image of Yaktocat](https://github.com/ting-chih/NTU-ML2021spring/blob/main/image/engtochi.png)  
